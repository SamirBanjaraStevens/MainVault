Method for *[[Orthonormalizing]]* a set of *[[Vectors]]* in an inner space,
	most commonly the *[[Euclidean space]]* $\mathbb{R}^{n}$ 
		equipped with the *[[Standard inner product]]* 

==The Gram-Schmidt process takes== a *[[Finite]]* 
	*[[linearly independent]]* set of vectors
		$S= \{ v_{1}, \dots , v_{2} \}$ for $k\leq n$ 
			and generates an *[[Orthogonal set]]* 
				$S'=\{ u_{1}, \dots, u_{2} \}$ 
					that spans the same $k$-dimensional subspace of $\mathbb{R}^{n}$ as $S$

The application of the Gram-Schmidt process 
	to the column vectors
		of a full column *[[Rank matrix]]* 
			yields the *[[QR decomposition]]* 
				(decomposed into an *[[orthogonal]]* and a *[[Triangular matrix]]*) 

## The Gram-Schmidt Process

We define the *[[Projection operator]]* by,
	$$\textbf{proj}_{u}(v)=\cfrac{\left< v, u \right> }{\left< u, u \right> } \ u$$
		where $\left< v, u \right>$ denotes the *[[inner product]]*
			of vectors $v$ and $u$

This operators projects the vector $v$
	orthogonally onto the lined 
		spanned by the vector $u$

If $u=0$ 
	we define $\textbf{proj}_{0}(v):=0$ 
			(i.e., the projection map $\textbf{proj}_{0}$ is the zero map)
		sending every vector to the zero vector

#### The Gram-Schmidt Process 
***
$$
\begin{align}
u_{1}&=v_{1}, &e_{1}&=\cfrac{u_{1}}{\|u_{1}\|}\\
u_{2}&=v_{2}-\textbf{proj}_{u_{1}}(v_{2}), &e_{2}&=\cfrac{u_{2}}{\|u_{2}\|}\\
u_{3}&=v_{3}-\textbf{proj}_{u_{1}}(v_{3})-\textbf{proj}_{u_{2}}(v_{3}), &e_{3}&=\cfrac{u_{3}}{\|u_{3}\|}\\
u_{4}&=v_{4}-\textbf{proj}_{u_{1}}(v_{4})-\textbf{proj}_{u_{2}}(v_{4})-\textbf{proj}_{u_{3}}(v_{4}), &e_{4}&=\cfrac{u_{4}}{\|u_{4}\|}\\
&\vdots &\vdots\\
u_{k}&=v_{k}-\sum_{j=1}^{k-1} \ \text{proj}_{u_{j}}(v_{k}), &e_{k}&=\cfrac{u_{k}}{\|u_{k}\|}\\
\end{align}
$$

The sequence 
	$u_{1}, \dots, u_{k}$ 
		is the required system of orthogonal
and the normalized vectors 
	$e_{1}, \dots, e_{k}$
		form an *[[Orthonormal]]* Set

The calcuations of the sequence
	$u_{1}, \dots, u_{k}$
		is known as the **Gram-Scmdit** *[[Orthogonalization]]* 
while the calculation of the sequence
	$e_{1}, \dots, e_{k}$ 
		is known as the **Gram-Schmidt** *[[Orthonormalizing|orthonormalization]]* 
			as the vectors as normalized

To check that these formulas yield an orthogonal sequence,
	first compute $\left< u_{1}, u_{2} \right>$
		by substituting the aboce formula for $u_{2}$
			:we get zero
Then use this to compute $\left< u_{1}, u_{3} \right>$ 
	again by substituting for $u_{3}$
		:we get zero
The general prood proceeds by *[[Mathematical induction]]*

#### Geometrically
***
Geometrically this method proceeds as follows:
	to compute $u_{i}$ 
		it projects $v_{i}$ orthogonally onto the subspace $U$
			generated by $u_{1}, \dots, u_{\ i-1}$ 
				which is the same as the subspace generated by $v_{1}, \dots, v_{\ i-1}$
The vector $u_{i}$ is then defined to be the difference between
	$v_{i}$ and this projection
		guaranteed to be orthogonal to all of the vectors in the subspace $U$

***

The Gram-Schmidt process 
	also applies to linearly independent 
		*[[Countably infinite]]* sequence $\{ v_{i} \}_{i}$ 
The result is an orthogonal (or orthonormal) sequence $\{ u_{i} \}_{i}$
	such that for a natural number $n$
		the algebraic span of $v_{1}, \dots, v_{n}$ 
			is the same as that of $u_{1}, \dots, u_{n}$

***

If the Gram-Schmidt process is 
	applied to a linearly dependent sequence, 
		it outputs the $0$ (zero) vector
			on the $i^{\text{th}}$ step
				assuming that $v_{i}$ is a linear combination of 
					$v_{1}, \dots, v_{i-1}$
If an orthonormal basis is to be produced,
	then the algorithm should test for zero vectors in the output 
		and discard them
			because no multiple of a zero vector 
				can have a lenght (norm) of $1$
The number of vectors output by the algorithm will then
	be the dimension of the 
		space spanned 
			by the original inputs.

***

A variant of the Gram-Schmidt process using *[[Transfinite recursion]]* 
	applied to a (possibly uncountably) infinite sequence of vectors
		$(v_{\alpha})_{\alpha<\lambda}$ 
			yield a set of orthonormal vectors
				$(u_{\alpha})_{\alpha<\mathcal{K}}$ with $\mathcal{K}\leq \lambda$
					such that for any $\alpha \leq \lambda$ 
						the *[[Completion]]* of the span of $\{ u_{\beta}: \beta< \textbf{min} \}(\alpha, \mathcal{K})$ 
							is the same as that of $\{ v_{\beta}:\beta<\alpha \}$
In particular, when applied to a (algebraic) basis 
	of a *[[Hilbert space]]* (or, more generally, a basis of any dense subspace)
		it yields a (functional-abalytic) orthonormal basis.
**Note:** that in the geberal case
			often the strick inequality $\mathcal{K}<\lambda$ holds,
				even if the starting set was linearly independent,
					and the span of $(u_{\alpha})_{\alpha<\mathcal{K}}$ 
						need not be a subspace 
							of the span of $(v_{\alpha})_{\alpha<\lambda}$
								(rather, it's subspace of its completion)

***

## Numerical Stability

When this process is implemented on a conputer, 
	the vectors $u_{k}$ are often not quite orthogonal
		due to *[[Rounding errors]]* 
For the Gram-Schmidt process (clasical)
	this loss of orthogonality is bad and is numerically ustable
The Classical Gram-Schmidt can be stabalized by a small modification,
	this version is the **Modified Gram-Schmidt** (*MGS*) 
This approach gives the same result as the originnal formula
	in exact arithemetic 
		and introduces smaller errors in finite-precison arithmetic
Insted of computing the vector $u_{k}$ as 
	$$u_{k}=v_{k}-\text{proj}_{u_{1}}(v_{k})-\text{proj}_{u_{2}}(v_{k})- \dots - \text{proj}_{u_{k-1}}(v_{k})$$
Given the vectors $v_{1}, v_{2}, \dots, v_{n}$ 
	in our first step we produce vectors $v_{1}, v_{2}^{(1)}, \dots, v_{n}^{(1)}$
		by removing components along the direction of $v_{1}$ 
In the formulas $v_{k}^{(1)}=v_{k}-\cfrac{\left< v_{k}, v_{1} \right>}{\left< v_{1}, v_{1} \right>}\ v_{1}$
After this step 
	we already have two 
		of our desired orthogonal vectors $u_{1}, \dots, u_{n}$
			namely, $u_{1}=v_{1}, \quad u_{2}=v_{2}^{(1)}$
				but, we also made $v_{3}^{(1)}, \dots, v_{n}^{(1)}$ 
					alredy orthogonal to $u_{1}$
Next, we orthogonalize those remaining vectors against $u_{2}=v_{2}^{(1)}$
This means we compute $v_{3}^{(2)}, v_{4}^{(2)}, \dots, v_{n}^{(2)}$ 
	by subbtraction $v_{k}^{(2)} = v_{k}^{(1)} - \cfrac{\left< v_{k}^{(1)}, u_{2} \right>}{\left< u_{2}, u_{2} \right>} \ u_{2}$
Now we have stored the vectors $v_{1}, v_{2}^{(1)}, v_{3}^{(2)}, v_{4}^{(2)}, \dots, v_{n}^{(2)}$
	where the first three vectors are already $u_{1}, u_{2}, u_{3}$
		and the remaining vectors are alredy orthogonal to $u_{1}, u_{2}$
As should be clear now, 
	the next step orthogonalizes $v_{4}^{(2)}, \dots, v_{n}^{(2)}$ 
		against $u_{3}=v_{3}^{(2)}$
Proceeding in this manner
	we find the full set of orthogonal vectors $u_{1}, \dots, u_{n}$
If orthonormal vectors are desired,
	then we normalize as we go
		so that the denominators in the subtraction formulas 
			turn into ones.

***

Gram-Schmidt process produces 
	the $j^{\text{th}}$ orthogonalized vector 
		after the $j^{\text{th}}$ iteration

While orthogonalization using the *[[Householder reflections]]* 
	produces all the vectors only at the end.

This makes Gram-Schmidt process applicable for *[[Iterative Methods]]* 
	like the *[[Arnoldi iteration]]* 



**Left of on last two parahraphs of**
https://www.wikiwand.com/en/Gram%E2%80%93Schmidt_process


