In this section we rely heavily on 
- Theorem 6.16 (p.369) and,
- Theorem 6.17 (p.371)
to develop an elegant representation of a **normal** (if $F=C$) 
- or a **self-adjoin**t (if $F=R$) **operator** $T$ on a finite-dimensional inner product space. 

We prove that $T$ can be written in the form 
$$\lambda_1T_2+\lambda_1T_2+\cdots+\lambda_kT_k$$
- where $\lambda_1, \lambda_2,\cdots, \lambda_k$ are the *distinct eigenvalues of $T$* 
- and $T_1, T_2, \cdots, T_k$ are  *orthogonal projections*

Take notes on Direct Sums at end of section 5.2
The special case where $V$ is a direct sum of two subspaces is considered in exercises of section 1.3 
- do some review

*Recall :*
That if $V = W_1 \, \oplus \, W_2$   
- then a linear operator $T$ on $V$ 
	- is the **Projection on $W_1$ along $W_2$** 
		- if whenever $x = x_1 + x_2$
		- with $x_1 \in W_1$ and $x_2\in W_2$
			- we have $T(x) = x_1$ 

By Exercise 27 of Section 2.1. we have 
$$R(T) = W_1 = \{x\in V : T(x) = x\} \text{ and } N(T) = W_2$$

So, $V = R(T) \, \oplus \, N(T)$ 

Thus, there is no ambiguity if we refer to $T$ as a "projection on $W_1$" 
or simply as a "projection".
In fact, 
- it can be show (from Exercise 17 of Section 2.3) 
	- that $T$ is a projection 
		- if and only if $T = T^2$ 
Because $V = W_1 \, \oplus \, W_2 = W_1 \oplus W_3$ 
- does not imply that 
	- $W_2 = W_3$
We see that $W_1$ does not uniquely determine $T$.
For an *orthogonal projection $T$*, 
- However, **$T$ is uniquely determined by its range.**


## **Definition :** 

Let $V$ be an inner product space, 
and let $T : V \rightarrow V$ be a projection (linear operator)

We say that $T$ is an **orthogonal projection** 
- if $R(T)^\perp = N(T)$ and $N(T)^\perp = R(T)$ 
	- by Exercise 13(c) of Section 6.2, if $V$ is finite-dimensional, 
		- we need only assume that one of the equalities in this definition holds.
			- For example:
			- $R(T)^\perp = N(T)$, then $R(T) = R(T)^{\perp\perp} = N(T)^\perp$ 

And orthogonal projection is not same as orthogonal operator.
In Figure 6.5 below, 
- $T$ is an orthogonal projection,
	- but $T$ is **not** a <u>orthogonal operator</u> 
		- because $\|T(v)\| \neq \|v\|$ 
			- ![[Pasted image 20230306230855.png]]

Now assume that $W$ is a finite-dimensional subspace 
- of an inner product space $V$.

In the notation of Theorem 6.6 (p. 347), 
- we can define a function $T: V \rightarrow V$  by $T(y) = u$ 

It is easy to show that $T$ is an orthogonal projection on $W$
- we can say even more
	- There exists exactly one orthogonal projection on $W$
		- For if $T$ and $U$ are orthogonal projections on $W$,
			- then, $R(T) = W = R(U)$ 
			- Hence, $N(T) = R(T)^\perp = R(U)^\perp = N(U)$ 
				- and, since **every projection is uniquely determined by its range and null space** 
				- we have $T = U$ 
					- we call $T$ the orthogonal projection of $V$ on $W$ 

To understand the geometric difference between 
- an arbitrary projection on $W$ 
- and the orthogonal projection on $W$ 
	- Let $V = R^2$ and $W = \text{span}\{(1, 1)\}$ 
	- Define $U$ and $T$ as in figure 6.5 above. 
		- Where $T(v)$ is the foot of a perpendicular from $v$ on the line $y = x$ 
		- and $U(a_1, a_2) = (a_1, a_2)$
			- Then, $T$ is the orthogonal projection of $V$ on $W$ 
			- and $U$ is a different projection on $W$.
- *Note :* 
	- $v - T(v)\in W^\perp$  
	- whereas, $v-U(v) \notin W^\perp$ 

- $T(v)$ is the "best approximation in $W$ to $v$ 
	- That is, if $w \in W$, 
		- then $\|w-v\|\geq \|T(v)-v\|$ 
			- In fact this approximation property characterizes $T$.
				- These results follow immediately from corollary to Theorem 6.6

As an application to Fourier analysis, 
- recall the inner product space $H$ and the orthonormal set $S$ from Example 9 of Section 6.1.



### **Theorem 6.24:**
Let $V$ be an inner product space, 
and let $T$ be a linear operator on $V$.
Then $T$ is an orthogonal projection 
- if and only if $T$ has an *[[2 Lecture#^c19a1e|adjoint]]* $T^*$ 
	- and $T^2 = T = T^*$

*Proof :*
Suppose that $T$ is an orthogonal projection. 
- Since $T^2 = T$ because $T$ is an projection, 
	- we only need to show that the adjoint $T^*$ exists and $T= T^*$ \
- Now $V = R(T) \oplus N(T)$  and $R(T)^\perp = N(T)$ 
	- Let $x, y \in V$ 
		- Then, we can write 
			- $x = x_1+x_2$ and $y = y_1+y_2$
				- where $x_1,y_2 \in R(T)$ and $x_2, y_2 \in N(T)$
					- Hence 
$$\langle x, T(y)\rangle = \langle x_1+x_2, y_1\rangle = \langle x_1, y_1\rangle + \langle x_2, y_1\rangle = \langle x_1, y_1\rangle$$
and 
$$\langle T(x), y)\rangle = \langle x_1,\, y_1+ y_2\rangle = \langle x_1, y_1\rangle + \langle x_1, y_2\rangle = \langle x_1, y_1\rangle$$
so, $\langle x, T(y)\rangle = \langle T(x), y\rangle$ for all $x, y \in V$ 
- Thus, $T^* exists and $T = T^*$ 

Now suppose that $T^2 = T = T^*$.
- Then, $T$ is a projection by Exercise 17 of Section 2.3.
	- and hence we must show that
		- $R(T) = N(T)^\perp$ 
		- and $R(T)^\perp = N(T)$ 
			- Let $x\in R(T)$ and $y\in N(T)$. 
				- then $x = T(x) = T(x)^*$ and so
$$\langle x, y\rangle = \langle T^*(x), \, y\rangle = \langle x,\, T(y)\rangle = \langle x,0\rangle = 0$$
- Therefore, $x \in N(T)^\perp$ 
	- from which it follow that $R(T)\subseteq N(T)^\perp$ 

- Let $y \in N(T)\perp$,
	- we must shoe that $y\in R(T)$ 
		- that is $T(y) = y$ 
	Now, 
$$
\begin{aligned}
\|y-T(y)\|^2 &= \langle y-T(y), y-T(y)\rangle\\
&= \langle \ y, y-T(y) \ \rangle \ - \ \langle \ T(y) , \ y-T(y) \ \rangle
\end{aligned}
$$
Since $y-T(y) \ in N(T)$
- the first term must equal zero.
	- But also,
$$
\begin{aligned}
\langle T(y),y-T(y)\rangle&=\langle y,T^*(y-T(y))\rangle\\
&= \langle y, T(y-T(y))\rangle\\
&= \langle y, 0 \rangle\\
&= 0
\end{aligned}
$$
- Thus, $y-T(y) = 0$ 
	- that is, $y = T(y)\in R(T)$

- Hence $R(T) = N(T)^\perp$ 
Using the preceding results, we have 
- $R(T)^\perp = N(T)^{\perp\perp} \supseteq N(T)$ by Exercise 13(b) of Section 6.2.

Now suppose that $x\in R(T)^\perp$ 
- for any $y \in V$, we have
$$
\begin{aligned}
\langle T(x), y\rangle &= \langle x, T^*(y)\rangle\\
&= \langle x, T(y)\rangle\\
&= 0
\end{aligned}
$$
so, $T(x) = 0$ 
- and thus, $x\in N(T)$
	- Hence $R(T)^\perp = N(T)$ 





Let $V$ be a finite-dimensional inner product space, 
$W$ be a subspace of $V$, and 
$T$ be the orthogonal projection of $V$ on $W$ 

We may chose an orthonormal basis $\beta = \{v_1, v_2, \cdots, v_n\}$ for $V$
- such that $\{v_1, v_2, \cdots, v_k\}$ is a basis for $W$ 

Then, $[T]_\beta$ is a diagonal matrix 
- with ones as the first $k$ diagonal entires and 
- zero elsewhere.
	- $[T]_\beta$ has the form 
		- $$\begin{pmatrix}I_k & O_1\\ O_2 & O_3\end{pmatrix}$$
If $U$ is any projection on $W$, 
- we may choose a basis $\gamma$  for $V$ such that $[U]_\gamma$ has the form above
	- However, $\gamma$ is not necessarily orthonormal

### **Theorem 6.25 (The Spectral theorem**

Suppose that $T$ is a linear operator (maps vector space onto itself)
- on a finite-dimensional inner product space $V$ over $F$ 
- with the distinct eigenvalues $\lambda_1, \lambda_2, \cdots, \lambda_k$

Assume that $T$ is normal if $F=C$ 
- and that $T$ is self-adjoint if $F=R$ 

For each $i \ (1\leq i\leq k)$ 
- let $W_i$ be the eigenspace of $T$ corresponding to the eigenvalue $\lambda_i$ 
- and let $T_i$ be the orthogonal projection of $V$ on $W$ 

<u>Then the following statements are true</u>

1. $V = W_1 \oplus W_2 \oplus \cdots \oplus W_k$
2. if $W'_i$ denotes the direct sum of the subspace $W_j$ for $j \neq i$ 
	- then, $W_i^{\perp}= W'_i$ 
3. $T_iT_j = \delta_{ij}T_i \quad \text{ for } 1\leq i , \ j\leq k$ 
4. $I = T_1 + T_2 + \cdots + T_k$
5. $T = \lambda_1\, T_1 + \lambda_2\, T_2 + \cdots + \lambda_k\, T_k$  

*Proof :* 
1. By Theorem 6.16 (p. 369) and Theorem 6.17 (p. 371)
	- $T$ is diagonalizable 
		- so, $V = W_1 \oplus W_2 \oplus \cdots \oplus W_k$ 
			- by Theorem 5.10 (p. 277)

2. If $x\in W_i$ and $y\in W_j$ for some $i \neq j$ 
	- then, $\langle x, y\rangle = 0$ 
		- by Theorem 6.15(d) (p. 368)
			- If follows, that $W'_i \subseteq W_i^\perp$ 
				- from (1.) we have,
				- $$\begin{aligned}\text{dim}(W'_i) &= \sum_{j\neq i} \text{dim}(W_j)\\ &=\text{dim}(V) - \text{dim}(W_i)\end{aligned}$$
				- on the other hand, we have $\text{dim}(W_i^\perp) = \text{dim}(V) - \text{dim}(W_i)$ 
					- by Theorem 6.7(c or 3) (p. 349)
				-  Hence, $W_i' = W_i^\perp$, proving (b)
3. Exercise
4. Since, $T_i$ is the orthogonal projection of $V$ on $W_i$ 
	- it follows from (2) that $$\begin{aligned}N(T_i) & = R(T_i)^\perp\\ &= W_i^\perp\\ &= W_i'\end{aligned}$$
	- Hence, for $x\in V$ we have $x = x_1 + x_2 + \cdots + x_k$ 
		- where $T_i(x) = x_i \in W_i$ Proving (4)
5. For $x\in V$, 
	- write $x = x_1 + x_2 + \cdots + x_k$, where $x_i \in W_i$ 
		- then, 
$$
\begin{aligned}
T(x) &= T(x_1) + T(x_2) + \cdots + T(x_k)\\
&= \lambda_1\,x_1 + \lambda_2\, x_2 + \cdots + \lambda_k \, x_k\\
&= \lambda_1\,T_1(x) + \lambda_2\,T_2(x) + \cdots + \lambda_k \, T_k(x)\\
&= (\lambda_1 \, T_1 + \lambda_2 \, T_2 + \cdots + \lambda_k \, T_k)(x)
\end{aligned}
$$
 $\{\lambda_1, \lambda_2, \cdots, \lambda_k\}$ is the set of eigenvalues of $T$ 
 - and is called the **Spectrum** of T

The sum $I = T_1 +T_2 + \cdots + T_k$ in (4.) 
- is called the **resolution of the identity** operator induced by $T$

and the sum $T = \lambda_1 \, T_1 + \lambda_2 \, T_2 + \cdots + \lambda_k \, T_k$ in (5.)
- is the **spectral decomposition** of $T$

The spectral decomposition of $T$ is unique up to the order(multiplicity) of its eigenvalues.

let $\beta$ be the union of orthonormal bases of the $W_i$'s 
and let $m_i = \text{ dim}(W_i)$ 
- thus, $m_i$ is he multiplicity of $\lambda_i$ 
	- Then $[T]_\beta$ has the form, 
$$
\begin{pmatrix}
\lambda_1I_{m_1} & O & \cdots & O\\
O & \lambda_2I_{m_2} & \cdots & O\\
\vdots & \vdots & \ddots & \vdots\\
O & O & \cdots & \lambda_kI_{m_k}
\end{pmatrix}
$$
- $[T]_\beta$ is a diagonal matrix in which the diagonal entries are he eigenvalues of $\lambda_i$ 
	- and each $\lambda_i$ is repeated $m_i$ times 
- If $\lambda_1 \, T_1 + \lambda_2 \, T_2 + \cdots + \lambda_k \, T_k$ is the **spectral decomposition** of $T$
	-  then it follows from (Exercise 7) that $g(T) = g(\lambda_1)T_1 + g(\lambda_2)T_2 + \cdots + g(\lambda_k)T_k$ for any polynomial $g$

:













