## Newtons Method and Its Extensions

Newtons (or the *Newton-Rapson*) method is a numerical method for solving root-finding problems.

Suppose that $f \in C^{2} [a,b]$
Let $p_{0} \in[a, b]$ be an approximation to $p$ 
	such that $f'(p_{0} \neq 0)$
	and $|p-p_{0}|$ is 'small'
Consider the first Taylor polynomial 
	for $f(x)$ expanded about $p_{0}$ 
		and evaluated at $x=p$
$$f(p) = f(p_{0})+(p-p_{0})f'(p_{0})+\cfrac{(p-p_{0})^{2}}{2}f''(\xi(p) )$$
where $\xi(p)$ lies between $p$ and $p_{0}$
Since $f(p) = 0$, this equation gives
$$
0 = f(p_{0}) + (p-p_{0})f'(p_{0})+\cfrac{(p-p_{0})^{2}}{2}f''(\xi(p))
$$
Newtons method is derived by 
	assuming that since $|p-p_{0}|$ is small,
		the term involving $(p-p_{0})^{2}$ is much smaller, so
$$
0 \approx f(p_{0})+(p-p_{0})f'(p_{0})
$$
	*Note: This equation is now a linear equation*
Solving for $p$ gives,
$$p \approx p_{0} - \cfrac{f(p_{0})}{f'(p_{0})} \equiv p_{1}$$
This sets the stage for Newton's method,
	which starts with an initial approximatin $p_{0}$
		and generates the sequence $\{ p_{n} \}_{n=0}^{\infty}$, by
$$
\tag{2.7}
p_{n} = 
p_{n-1} - \cfrac{f(p_{n-1})}{f'(n-1)}, \quad \text{ for } \geq 1
$$
The approximationsare obtained using succesive tangents.
Starting with the initial approximation $p_{0}$,
	the approximation $p_{1}$ is the $x$-intercept of the tangent line
		to the graph of $f$ at $(p_{0},f(p_{0}))$ 
The approximation $p_{2}$ is the $x-$intercept of the tangent line to the graph of $f$ at $(p_{1},f(p_{1}))$
##### Algorithm 2.3 Newton's Method

```
INPUT : Intial approximation p_0; tolerance 'TOL'; maximum number of iterations N_0
OUTPUT : Approximate solution p or message of failure.

Step 1,    Set i = 1
Step 2,    While i \leq N_0 do Steps 3-6
	Step 3,    Set p = p_0 - f(p_0)/f'(p0)    (Compute p_i)
	Step 4,    It |p-p_0| < TOL then
			       OUTPUT (p);
			       Stop.
	Step 5,    Set i=i+1
	Step 6,    Set p0 = p    (Update p_0)
	Step 7,    OUTPUT    ('The method failed after N_0 iterations, N_0 =' N_0);
	                     (The procedure was unsuccessful.)
			   STOP		
```
The stopping-technique inequalities given with the *Bisection method*
	are applicable to Newton's method.
That is, select a tolerance $\epsilon > 0$ and construct $p_{1}, \dots p_{n}$ until
$$
\begin{align}
\tag{2.8} | \ p_{N} - p_{N-1} \ | &< 0 \epsilon, \\
\tag{2.9}\cfrac{| \ p_{N}-p_{N-1} \ |}{| \ p_{N} \ |} &< \epsilon, \quad p_{N} \neq \\
 \\
\text{ or }  \\
 \\
\tag{2.10} | \ f(p_{N}) \ | &< \epsilon\\
\end{align}
$$
A form of inequality *(2.8)* is used in step 4 of *Algorithm 2.3*
Note that none of the inequalities *(2.8), (2.9), or (2.10)*
	give precise information about the actual error $| \ p_{n} - p \ |$ 

Newton's method is a functional iteration technique with $p_{n} = g(p_{n-1})$, for which
$$
\tag{2.11}
g(p_{n-1}) = 
p_{n-1} - \cfrac{ f(p_{n-1})}{f'(p_{n-1})}, \quad \text{ for } n \geq 1
$$

In fact, this is the functional iteration technique that was used to give rapid convergence we saw in *column (e)* of Table 2.2 in Section 2.2

Newtons method can not be continued is $f'(p_{n-1}) = 0$ for some $n$
In fact, we will see that the *method* 
	is **most effective** 
		when $f'$ is *bounded* away from zero near $p$

###### Example 1
Consider the function $f(x) = \cos x - x = 0$ 
Approximate a root of $f$ using 
	*(a)* a fixed=point method,
	and *(b)* Newton's method

**Solution :**

*(a)* A solution to this root-finding problem is also a solution to the fixed-point problem $x = \cos x$ also a single fixed point lies in 
$\left[ 0, \frac{\pi}{2} \right]$
	Best we can conclude with $p_{0}=\cfrac{\pi}{4}$ and $p \approx 0.74$ 

*(b)* To apply Newtons method to this problem we need $f'(x) = -\sin x - 1$ 
	Starting again with $p_{0}=\cfrac{\pi}{4}$ we have
$$
\begin{align}
p_{1}  & = p_{0} - \cfrac{p_{0}}{f'(p_{0})} \\
 & = \cfrac{\pi}{4} - \cfrac{\cos\left(\cfrac{\pi}{4}-\cfrac{\pi}{4}\right)}{- \sin \left( \cfrac{\pi}{4}  \right) - 1 } \\
 &  = 0.7395361337 \\
p_{2} & = p_{1} - \cfrac{\cos(p_{1})-p_{1}}{-\sin(p_{1})-1} \\
& = {7390851781}
\end{align}
$$
we continue by generating the sequence by 
$$p_{n}=p_{n-1}-\cfrac{f(p_{n-1})}{f'(p_{n-1})} = p_{n-1}-\cfrac{\cos(p_{n-1})-p_{n-1}}{-\sin(p_{n-1})-1}$$
This gives the approximations. An excellent approximation is obtained with $n=3$
Because of the agreement of $p_{3}$ and $p_{4}$, we could reasonably expect this result to be accurate to the places listed.

## Convergence Using Newton's Method

Example 1 shows that Newton's method can
	provide extremely accurate approximations
		 with very few iterations.
For that example, only one iteration of Newton's method
	was needed to give better accuracy
		 than seven iterations of the fixed-point method.

Lets examine Newtons method more carefully to discover why its so effective.

The Taylor series derivation of Newton's method
	at the beginning of the section points out
		 the importance of an accurate initial approximation.
The crucial assumption is that the term involving $(p-p_{0})^{2}$ is,
	by comparison with $|\ p-p_{0} \ |$, 
		so small that it can be deleted.
This will clearly be false unless $p_{0}$ is a good approximation to $p$. 
If $p_{0}$ is not sufficiently close to the actual root,
	there is little reason to suspect that Newtons method will converge to the root.
However, in some instances, even poor initial approximations will produce convergence. 
***
The following convergence theorem for Newtons method illustrates the theoretical importance of the choice of $p_{0}$	
### Theorem 2.6

Let $f \in C^{2}[a, b]$

If $p \in(a, b)$ such that $f(p) = 0$ and $f'(p)\neq 0$
	then, there exists a $\delta > 0$ 
		such that Newtons method generates a sequence $\{ p_{n} \}_{n=1}^{\infty}$ 
			converging to $p$ for any initial approximation $p_{0} \in [p - \delta, \ p + \delta]$

**PROOF:**

Based on analyzing Newtons method
	as the functional iteration scheme
		$p_{n} = g(p_{n-1})$, for some $n \geq 1$ with 
$$
g(x) = x-\cfrac{f(x)}{f'(x)}
$$
Let $k$ be in $(0, 1)$ 
We first find an interval $[p-\delta, \ p + \delta]$
	that $g$ maps into itself 
		and for which $|\,g'(x) \,| \leq k$ for all $x \in (p-\delta, \ p + \delta)$
Since $f'$ is continuous on $[p-\delta, \ p + \delta]$ and $f'(p) \ne 0$
	part *(a)* of exercise 30 in Section 1.1 
		implies that there exists a $\delta_{1} > 0$, 
			such that $f'(x) \ne 0$ for $x \in[p-\delta, \ p + \delta] \subseteq [a, b]$ 
Thus, $g$ is defined and continuous on $[p-\delta, \ p + \delta]$. Also, 
$$
\begin{align}
g'(x) &= 1-\cfrac{f'(x)f'(x)-f(x)f''(x)}{[f'(x)]^{2}}\\ &=\cfrac{f(x) f''(x)}{[f'(x)]^{2}}\\
\end{align}
$$
	for $x \in [p-\delta, \ p + \delta]$ 
		and since $f \in C^{2}[a, b]$, 
			we have $g \in C^{1}[p-\delta, \ p + \delta]$
By assumption, $f(p)=0$, so
$$g'(p)=\cfrac{f(p)f''(p)}{[f'(p)]^{2}}=0$$
Since, $g'$ is continuous and $0<k<1$
	part *(b)* of exercise 30 in Section 1.1 
		implies that there exists a $\delta$,
			with $0< \delta < \delta_{1}$,
				for which 
$$|\,g'(x)\,| \leq k \text{, for all } x \in [p-\delta, \ p + \delta]$$

Remains to show $g$ maps $[p-\delta, \ p + \delta]$ into $[p-\delta, \ p + \delta]$.

If $x \in [p-\delta, \ p + \delta]$, 
	the *Mean Value Theorem* implies
		that for some number $\xi$ between $x$ and $p$,
			$|\, g(x) - g(p)\,| = \left| g'(\xi) \right| \left| x-p \right|$ so,
$$
\begin{align}
\left|\, g(x)-p \,\right|  &= \left|\, g(x)-g(p) \,\right| \\
&= \left|\, g'(\xi) \,\right| \left|\, x-p \,\right|  \\
&\leq k \left|\, x-p \,\right| \\
&< \left|\, x-p \,\right| 
\end{align}
$$
Since, $x \in[p-\delta, \ p + \delta]$
	it follows that $\left|\, x-p \,\right| < \delta$ 
		and that $\left|\, g(x)-p \,\right| < \delta$
Hence, $g$ maps $[p-\delta, \ p + \delta]$ into $[p-\delta, \ p + \delta]$

All the hypothesis of the *Fixed-Point Theorem 2.4* are now satisfied,
	so the sequence $\{ p_{n} \}_{n=1}^{\infty}$, defined by
$$
\begin{align}
p_{n} &= g(p_{n-1}) \\
	&=p_{n-1}-cfrac{f(p_{n-1})}{f'(p_{n-1})} \quad \text{ for } n \geq 1,
\end{align}
$$
        converges to $p$ for any $p_{0} \in[p-\delta, \ p + \delta]$

Theorem *2.6* states that, 
		under reasonable assumptions
	Newtons method converges,
		provided that a sufficiently accurate initial approximation is chosen.
It also implies that the constant $k$ 
	that bounds the derivative of $g$ 
			and, consequently, 
		indicates the speed of the convergence of the method
			 decreases to $0$ as the procedure continues
This result is important for the theory of Newton method,
	but it is seldom applied in practice 
		because it *does not tell us* **how to determine** $\delta$
In a practical application,
	an initial approximation is selected,
		and successive approximations are generated by Newton's method.
Either these will generally converge quickly to the root
	or it will be clear that convergence is unlikely.


## The Secant Method

Newton's method is an extremely powerful technique, 
	but it also has a major **weakness**:
		<u>the need to know the</u> 
	*value of the derivative* of $f$
		**at each approximation.**
Frequently, $f'(x)$ is far more difficult 
	and needs more arithmetic operations
		to calculate $f(x)$
***
To circumvent the problem of the derivative evaluations
	in newtons method we introduce a slight variation. 

By definition, 
$$
f'(p_{n-1}) = \lim_{ x \to p_{n-1} } \cfrac{f(x)-f(p_{n-1})}{x-p_{n-1}}
$$
If $p_{n-2}$ is close to $p_{n-1}$ then
$$
f'(p_{n-1}) \approx \cfrac{f(p_{n-2})-f(p_{n-1})}{p_{n-2}-p_{n-1}} = \cfrac{f(p_{n-1})-f(p_{n-2})}{p_{n-1}-p_{n-2}}
$$
using this approximation for $f'(p_{n-1})$ in newtons formula gives
$$
\tag{2.12}
p_{n} = p_{n-1}-\cfrac{f(p_{n-1})(p_{n-1}-p_{n-2})}{f(p_{n-1})-f(p_{n-2})}
$$
##### Algorithm 2.4 Secant Method

###### Example 2


## The Method of False Position

Each successive pair of approximations in the **Bisection method** brackets a root $p$ of the equation;
	that is, for each positive integer $n$,
		a root lies between $a_{n}$ and $b_{n}$
This implies that, for each $n$, the **Bisection method** iterations satisfy, 
$$\left|\, p_{n} - p \,\right| < \cfrac{1}{2}\left|\, a_{n}-b_{n} \,\right| $$
	which provides an easily calculated error bound for the approximations.

Root bracketing is not guaranteed
	for either **Newton's method** or the **Secant method**. 
The **method of False Position** (also called *Regula Falsi*)
	Generates approximations in the same manner as the **Secant method**,
	But includes a test to ensure that the root is always bracketed between successive iterations.
Although it is not a method we generally recommend. 
	it illustrates how bracketing can be incorprated.
***
First, choose initial approximations $p_{0}$ and $p_{1}$ with $f(p_{0})\cdot f(p_{1})<0$ 
The approximation $p_{2}$ is chosen in the same manner as in the **Secant method** 
	as the $x$-intercept of the line joining $(p_{0},f(p_{0}))$ and $(p_{1}, f(p_{1}))$.
To decide which secant line to use to compute $p_{3}$ 
	consider $f(p_{2}) \cdot f(p_{1})$
		or more correctly 
			 $\mathrm{sgn}f(p_{2})\cdot \mathrm{sgn} f(p_{1})$ 
				1. *If* $\mathrm{sgn}f(p_{2})\cdot \mathrm{sgn}f(p_{1})<0$ 
					then, $p_{1}$ and $p_{2}$ bracket a root.
						Choose $p_{3}$ as the $x$-intercept of the line joining $(p_{1},f(p_{1}))$ and $p_{2},f(p_{2})$ 
				2. **If not** 
					Choose $p_{3}$ as the $x$-intercept
						of the line joining $(p_{0}, f(p_{0}))$ and $(p_{2}, f(p_{2}))$ 
							and then interchanging the indices on $p_{0}$ and $p_{1}$
In a similar manner, once $p_{3}$ is found
	the sign of $f(p_{3})\cdot f(p_{2})$ determines 
		whether we use $p_{2}$ and $p_{3}$ 
		or $p_{3}$ and $p_{1}$ 
			to compute $p_{4}$
In the latter case a relabeling of $p_{2}$ and $p_{1}$ is performed.
The relabeling ensures that the root is bracketed between successive iterations.
##### Algorithm 2.5

To find a solution to $f(x) = 0$,
	given the continuous function $f$
		on the interval $[p_{0}, p_{1}]$ 
			where $f(p_{0})$ and $f(p_{1})$ have opposite signs:
```
Input : Initial approximations : p_0, p_1
		Tolerance : TOL
		Maximum number of iterations : N_0
		
Output : Approximate solution p or message of failure

i = 2
q_0 = f(p_0)
q_1 = f(p_1)
While i <= N_0
	p = p_1 - (q_1 * (p_0 - p_1) / (q_0 - q_1))   \\ (Compute p_i)
	
	if |p - p_1| < TOL
		OUTPUT(p)    \\ (The procedure was successful)
		STOP.
	
	i = i + 1
	q = f(p)
	
	if q * q_1 < 0 
		p_0 = p1
		q_0 = q_1
	p_1 = p
	q_1 = q
OUTPUT ('Method failed after N_0 iterations, N_0 = N_0');    
\\ (The procedure was unsuccessful)
```

$\mathbb{R} \arctan \sqrt{ rt }$
###### Example 3


The added insurance of the method of **False Position** commonly requires
	more calculations than the *Secant method*

Just as the simplification that the *Secant method* provides
	over **Newton's method** comes as the expense of additional iterations.

Lets try to experiment with Complex Numbers.

  

There is an issue with the use of absolute value of Real Numbers

$\mathbb{R}$.

We need to use the modulus of complex numbers defined as follows,

Let $z$ be a complex number represented in the standard form as

$z = a+bi$

Where $a$ and $b$ are $\mathbb{R}$ real numbers,

and $i$ is the imaginary unit such that $i^2 = -1$

  

The modulus (or magnitude) of $z$,

is the negative square root of the sum of squares of its real and imaginary parts, denoted, $|z| = \sqrt{a^2+b^2}$

With the following properties:

1. $|z| \geq 0$ for all complex numbers $z$, and $|z| = 0$ if and

only if $z = 0$.

2. $|z_1z_2| = |z_1| \, |z_2|$ for all complex number $z_1$ and $z_2$.

3. $|\hat{z}| = |z|$, where $\hat{z}$ is the complex conjugate of $z$.

4. $|z_1+z_2| \leq |z_1|+|z_2|$ (Triangle Inequality) for all complex numbers $z_1$ and $z_2$

  

And the argument is defined as follows:

  

The argument of $z$, denoted by $\mathrm{arg}(z)$ or $\theta$, is the angle (in radians) that the line segment joining the origin and the point representing $z$ in the complex plane makes with the positive real axis. The angle is measured in the counter clockwise direction.

  

The *principal value* of the argument is given by:

$\mathrm{arg}(z) = \arctan(\cfrac{b}{a})$

  

The quadrant in which $z$ lies must be taken into account to determine the correct angle:

  

1. if $a > 0$ and $b \geq 0$

then, $\mathrm{arg}(z) = \arctan(\cfrac{b}{a})$

2. if $a<0$,

then $\mathrm{arg}(z) = \arctan\left(\cfrac{b}{a}\right) + \pi$

3. if $a < 0$

4. if $a = 0$ and $b\neq 0$

then $\mathrm{arg}(z) = \frac{\pi}{2}$ for $b >0$

and $\mathrm{arg}(z)=-\frac{\pi}{2}$ for $b < 0$

  
  

We need to modify the code, to reflect the solutions shown below.

1. Replace all the absolute value computations with the modulus computations for complex numbers. (we can still use `abs()`)

  

2. Instead of checking if `f(a) * f(b) > 0`

we can check if the argument of the two complex number is different by more than the `TOL`.

This ensure they lie in different half-planes.