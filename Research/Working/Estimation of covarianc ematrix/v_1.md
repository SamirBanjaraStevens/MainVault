Let $X_{1}, X_{2},\dots,X_{n}$ be $n$ independent and identically distributed copies of a $p$-dimensional Gaussian random vector $X_{i}=(X_{i1}, X_{i2},\dots, X_{ip})^{T} \textasciitilde N(\mu,\Sigma)$. We assume $\Sigma$ has full rank $p$

The goal is to estimate the covariance matrix $\Sigma$ and its inverse $\Omega$ based on the sample $\{ X_{i} : i=1,2,\dots,n\}$ . 


Maximum Likihood Estimator (Empirical covariance matrix)
***
The sample covariance matrix is defined as follows,
$$
\Sigma_{X} = \frac{1}{n-1}\sum_{i=1}^{n} \ (X_{i}-\mu_{X})(X_{i}-\mu_{X})^T
$$
where $\mu_{X}=\cfrac{\sum_{i=1}^nX_{i}}{n}$ is the sample mean

This is not a consistent estimator of the covariance matrix $\Sigma$ when $p\gg n$, and structural assumptions are required in order to estimate $\Sigma$ consistently. 

Let's consider the case $X_{i}\textasciitilde N(0,\Sigma)$. $\Sigma_{X}$ is a consistent estimator. It is simple to construct, unbiased, and intuitive. 

However, when $n<p$ the rank is at most $n$ thus $\Sigma_{X}$ is not invertible, in the sense that $p/n\to c\in(0,\infty)$. 

Additionally, when $n\geq p$, $\Sigma_{X}$ has significant sampling error, in terms of $||\cdot||_{2}$ (forbinus norm), thus $\Sigma_{X}$ is not consistent and so its inverse is a poor estimator for $\Omega$. While $\Sigma_{X}$ is unbiased for $\Sigma$, $\Sigma_{X}^{-1}$ is highly biased for $\Sigma^{-1}$ if $p$ is close to $n$ 

For $\Sigma=I_{p}$, the empirical spectral density -distribution of the eigenvalues- converges to large distribution.

If $p/n\to c(0,1)$ $\Sigma_{X}$ is supported on $((1-\sqrt{ c }^{2}), (1+{\sqrt{ c }^{2}}))$.

Additionally, If $\frac{p}{n}\to c >0$ the smallest and largest eigenvalues of $\Sigma_{X}$ will converge to $\mathrm{max}(0, (1-\sqrt{ c }^{2}))$ and $\mathrm{max}(0,(1+\sqrt{ c }^{2}))$ 
***
In general, structural assumptions of $\Sigma$ or $\Omega$ are needed for consistent estimation. 

In the following sections, we will examine methods with structural assumptions that estimate the covariance matrices that avoid undesirable properties of the sample covariance matrix. And their application in financial setting. 

Estimators include regularization techniques such as (tapering, banding, thresholding), principal component method, maximum likelihood method, modified Cholesky decomposition with regularization.

Another branch of estimator we will be investigating assume $\Sigma$ does not have diverging eigenvalues as $n,p \to \infty$. Focus is not on the estimators conforming to any structures on $\Sigma$, but shrinking the eigenvalues of $\Sigma_{X}$. These estimators include the shrinkage method, Bayesian & empirical Bayes estimators, and the method based on random matrix theory. 



## Tapering

## Banding
***
## Thresholding

## Principal Component Method
***
Let $X_{1}, X_{2},\dots,X_{n}$ be $n$ independent and identically distributed copies of a $p$-dimensional Gaussian random vector $X_{i}=(X_{i1}, X_{i2},\dots, X_{ip})^{T} \textasciitilde N(\mu,\Sigma)$. We assume $\Sigma$ is positive definite.

The goal is to estimate the covariance matrix $\Sigma$ and its inverse $\Omega$ based on the sample $\{ X_{i} : i=1,2,\dots,n\}$ . 

The sample variance-covariance matrix,
$$
\Sigma_{X} = \frac{1}{n-1}\sum_{i=1}^{n} \ (X_{i}-\mu_{X})(X_{i}-\mu_{X})^T
$$
We have $p$ eigenvalues and eigenvectors for $\Sigma_{X}$.

Eigenvalues of $\Sigma_{X}$
$$
\hat{\lambda}_{1}, \hat{\lambda}_{2},\dots, \hat{\lambda}_{p}
$$
and eigenvectors 
$$\hat{\mathrm{e}}_{1},\hat{\mathrm{e}}_{2}, \dots, \hat{\mathrm{e}}_{p}$$
$\Sigma_{X}$ can be decomposed as a function of the eigenvalues and eigenvectors. It follows from the spectral theorem of linear algebra that a positive-definite symmetric matrix $\Sigma_{X}$ has a unique positive-definite symmetric square root $\sqrt{ \Sigma_{X} }$.

Let $D$ be the $p\times p$ diagonal matrix formed by the diagonal elements of $\Sigma_{X}$. $D$ is a matrix of the specific ariances
Define $B = D^{-\frac{1}{2}}\Sigma_{X}D^{-1/2}$, so that the $B$ is the sample correlation matrix.

$B$ can be written as 
$$
\begin{align}
\Sigma = \sum_{i=1}^{p}\ \lambda_{i}\mathrm{e}_{}
\end{align}
$$


Spectral decomposition is used to factor $\Sigma_{X}$ into,
$$
\Sigma_{X}=CDC^T
$$
$C$ is an orthogonal matrix of the normalized eigenvectors of $\Sigma_{X}$ and $D$ is a diagonal matrix with the diagonal equaling teigenvaluesues of $\Sigma_{X}$. 






## Maximum Likelihood Method
## Modified Cholesky Decomposition With Regularization

## The Shrinkage Method



## Factor Models

## Bayesian & Empirical Bayes Estimators

## The Method Based on Random Matrix Theory
